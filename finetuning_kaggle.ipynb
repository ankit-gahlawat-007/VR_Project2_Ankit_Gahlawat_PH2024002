{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom transformers import ViltProcessor, ViltForQuestionAnswering, ViltConfig\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ============================\n# 1. Load JSON and Preprocess\n# ============================\n\nwith open(\"/kaggle/input/json-train/qna_train.json\") as f:\n    raw_data = json.load(f)\n\n# Get unique answers and define label mappings\nunique_answers = sorted({d[\"answer\"] for d in raw_data})\nlabel2id = {label: i for i, label in enumerate(unique_answers)}\nid2label = {i: label for label, i in label2id.items()}\n\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n# ============================\n# 2. Custom Dataset\n# ============================\n\nfrom torchvision import transforms\n\nclass CustomVQADataset(Dataset):\n    def __init__(self, data, processor, label2id):\n        self.data = data\n        self.processor = processor\n        self.label2id = label2id\n        self.resize = transforms.Resize((384, 384))  # ViLT expects 384x384\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        image_path = \"/kaggle/input/train-data\" + item[\"image_path\"]\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.resize(image)  # Resize image to fixed size\n        question = item[\"question\"]\n        answer = item[\"answer\"]\n\n        encoding = self.processor(\n            images=image,\n            text=question,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True\n        )\n        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n\n        label_id = self.label2id[answer]\n        encoding[\"labels\"] = torch.tensor(label_id).long()  # âœ… use index not one-hot\n\n        return encoding\n\nfrom transformers import ViltForQuestionAnswering\nimport torch.nn as nn\n\nclass ViltForSingleLabelClassification(ViltForQuestionAnswering):\n    def forward(self, *args, **kwargs):\n        labels = kwargs.pop(\"labels\", None)\n        output = super().forward(*args, **kwargs)\n        if labels is not None:\n            logits = output.logits\n            loss = nn.CrossEntropyLoss()(logits, labels)\n            return type(output)(loss=loss, **{k: v for k, v in output.items() if k != \"loss\"})\n        return output\n\n\n\n# ============================\n# 3. Model + LoRA\n# ============================\n\nconfig = ViltConfig.from_pretrained(\n    \"dandelin/vilt-b32-finetuned-vqa\",\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\nbase_model = base_model = ViltForSingleLabelClassification.from_pretrained(\n    \"dandelin/vilt-b32-finetuned-vqa\",\n    config=config,\n    ignore_mismatched_sizes=True\n)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS\n)\n\nmodel = get_peft_model(base_model, lora_config)\n\n# ============================\n# 4. Training\n# ============================\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\ndataset = CustomVQADataset(raw_data, processor, label2id)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\nmodel.train()\nfor epoch in range(15):\n    total_loss = 0\n    for batch in tqdm(dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"vilt-finetuned-vqa\")\nprocessor.save_pretrained(\"vilt-finetuned-vqa\")\nmodel.config.save_pretrained(\"vilt-finetuned-vqa\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}